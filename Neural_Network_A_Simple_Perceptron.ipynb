{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network - A Simple Perceptron\n",
        "\n",
        "## Question 1: What is Deep Learning? Briefly describe how it evolved and how it differs from traditional machine learning.\n",
        "- Deep Learning is a subset of Machine Learning that uses artificial neural networks with multiple layers to automatically learn complex patterns from data.\n",
        "- It evolved from Artificial Neural Networks (ANNs) inspired by the human brain.\n",
        "- Early models like the Perceptron (1958) laid the foundation, but computational limitations slowed progress.\n",
        "- With big data, GPUs, and improved algorithms, deep learning became widely successful in tasks like computer vision, speech recognition, and natural language processing.\n",
        "\n",
        "### Difference from Traditional ML:\n",
        "- Feature Engineering:\n",
        "    - Traditional ML → requires manual feature extraction.\n",
        "    - Deep Learning → automatically extracts features.\n",
        "\n",
        "- Scalability:\n",
        "    - Traditional ML → struggles with very large datasets.\n",
        "    - Deep Learning → performs better with huge datasets.\n",
        "\n",
        "- Performance:\n",
        "    - Traditional ML → effective on structured/tabular data.\n",
        "    - Deep Learning → excels in unstructured data (images, audio, text).\n",
        "\n",
        "## Question 2: Explain the basic architecture and functioning of a Perceptron. What are its limitations?\n",
        "- Architecture:\n",
        "   - Inputs → Weighted Sum → Activation Function → Output\n",
        "- Functioning:\n",
        "   - Takes multiple input values.\n",
        "   - Multiplies each by a weight and adds a bias.\n",
        "   - Passes the sum through an activation function.\n",
        "   - Produces an output (0/1 for binary classification).\n",
        "\n",
        "- Limitations:\n",
        "   - Can only solve linearly separable problems (fails on XOR).\n",
        "   - Limited learning capacity (single-layer).\n",
        "   - Sensitive to feature scaling.\n",
        "\n",
        "## Question 3: Describe the purpose of activation function in neural networks. Compare Sigmoid, ReLU, and Tanh functions.\n",
        "- Purpose: Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns.\n",
        "- Comparison:\n",
        "\n",
        "| Function    | Formula                                    | Range   | Pros                              | Cons                     |\n",
        "| ----------- | ------------------------------------------ | ------- | --------------------------------- | ------------------------ |\n",
        "| **Sigmoid** | $f(x) = \\frac{1}{1+e^{-x}}$                | (0, 1)  | Smooth, probabilistic output      | Vanishing gradient       |\n",
        "| **ReLU**    | $f(x) = \\max(0, x)$                        | (0, ∞)  | Fast, reduces vanishing gradient  | Dead neurons             |\n",
        "| **Tanh**    | $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | Zero-centered, stronger gradients | Still vanishing gradient |\n",
        "\n",
        "\n",
        "## Question 4: What is the difference between Loss function and Cost function in neural networks? Provide examples.\n",
        "- Loss Function:\n",
        "  - Measures error for a single training example.\n",
        "  - Example: Mean Squared Error (MSE) for regression, Binary Cross-Entropy for classification.\n",
        "\n",
        "- Cost Function:\n",
        "  - Average of the loss functions over the entire dataset.\n",
        "  - Key Difference: Loss → single instance, Cost → whole dataset.\n",
        "\n",
        "## Question 5: What is the role of optimizers in neural networks? Compare Gradient Descent, Adam, and RMSprop.\n",
        "\n",
        "| Optimizer            | Key Idea                                 | Pros                    | Cons                             |\n",
        "| -------------------- | ---------------------------------------- | ----------------------- | -------------------------------- |\n",
        "| **Gradient Descent** | Updates weights using gradient           | Simple, stable          | Slow, sensitive to learning rate |\n",
        "| **RMSprop**          | Uses moving average of squared gradients | Good for RNNs, adaptive | May converge too fast            |\n",
        "| **Adam**             | Combines Momentum + RMSprop              | Fast, widely used       | Slightly more memory usage       |\n"
      ],
      "metadata": {
        "id": "AyHtRASKhJO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Implement a single-layer perceptron from scratch (NumPy) for AND gate."
      ],
      "metadata": {
        "id": "YWV41XuGiZup"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m67CBJa9g8Lf"
      },
      "outputs": [],
      "source": [
        "# Solution 6\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# AND gate dataset\n",
        "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Initialize weights & bias\n",
        "w = np.zeros(X.shape[1])\n",
        "b = 0\n",
        "lr = 0.1\n",
        "\n",
        "# Training perceptron\n",
        "for epoch in range(20):\n",
        "    for i in range(len(X)):\n",
        "        linear_output = np.dot(X[i], w) + b\n",
        "        y_pred = 1 if linear_output >= 0 else 0\n",
        "        error = y[i] - y_pred\n",
        "        w += lr * error * X[i]\n",
        "        b += lr * error\n",
        "\n",
        "print(\"Weights:\", w, \"Bias:\", b)\n",
        "\n",
        "# Testing\n",
        "for xi in X:\n",
        "    print(xi, \"=>\", 1 if np.dot(xi, w) + b >= 0 else 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Implement and visualize Sigmoid, ReLU, and Tanh activation functions."
      ],
      "metadata": {
        "id": "30aFwiEkibkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 7\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "\n",
        "sigmoid = 1 / (1 + np.exp(-x))\n",
        "relu = np.maximum(0, x)\n",
        "tanh = np.tanh(x)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x, sigmoid, label=\"Sigmoid\")\n",
        "plt.plot(x, relu, label=\"ReLU\")\n",
        "plt.plot(x, tanh, label=\"Tanh\")\n",
        "plt.legend()\n",
        "plt.title(\"Activation Functions\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ER6wtcytisfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Use Keras to build a multilayer NN on MNIST dataset."
      ],
      "metadata": {
        "id": "c24I99IEjBZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 8\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "print(\"Training Accuracy:\", history.history['accuracy'][-1])"
      ],
      "metadata": {
        "id": "hKprY7Xoi0Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Visualize loss & accuracy curves for Fashion MNIST."
      ],
      "metadata": {
        "id": "flwMe83DjFkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 9\n",
        "\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28,28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))\n",
        "\n",
        "# Plot curves\n",
        "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "plt.plot(history.history['accuracy'], label=\"Training Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Loss & Accuracy Curves\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "useiIJBHi3kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Fraud Detection Workflow (Banking Project).\n",
        "\n",
        "- Model Design:\n",
        "  - Use a Multilayer Neural Network (MLP) with input layer → hidden layers (ReLU) → output (Sigmoid for fraud/legit).\n",
        "\n",
        "- Activation & Loss:\n",
        " - Hidden Layers → ReLU\n",
        " - Output Layer → Sigmoid\n",
        " - Loss Function → Binary Cross-Entropy (best for binary classification).\n",
        "\n",
        "- Training & Evaluation:\n",
        " - Handle class imbalance with SMOTE or class weights.\n",
        " - Evaluate with Precision, Recall, F1, AUC.\n",
        "\n",
        "- Optimizer & Overfitting Prevention:\n",
        " - Use Adam optimizer.\n",
        " - Apply Dropout and EarlyStopping."
      ],
      "metadata": {
        "id": "hgBdXhVpjMj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 10\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced',\n",
        "                                     classes=np.unique([0,1]),\n",
        "                                     y=[0,0,1,0,1,1,0]) # example\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy','AUC'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=128,\n",
        "          validation_data=(X_val, y_val), class_weight={0: class_weights[0], 1: class_weights[1]})"
      ],
      "metadata": {
        "id": "f3pN6nOIi70I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}